<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Auticuro Blog</title>
        <link>https://financial_platform.pages.awx.im/Auticuro/blog</link>
        <description>Auticuro Blog</description>
        <lastBuildDate>Wed, 19 Apr 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How to design a wallet service]]></title>
            <link>https://financial_platform.pages.awx.im/Auticuro/blog/How to design a wallet service</link>
            <guid>https://financial_platform.pages.awx.im/Auticuro/blog/How to design a wallet service</guid>
            <pubDate>Wed, 19 Apr 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[1. Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-introduction">1. Introduction<a href="#1-introduction" class="hash-link" aria-label="Direct link to 1. Introduction" title="Direct link to 1. Introduction">​</a></h2><p>With the rise of e-commerce, mobile payments, and digital currencies, people have become more reliant on digital transactions. Traditional cash-based payment has its limitations. Cash is not always practical or safe to carry around.</p><p>A wallet service is a software application that allows users to manage their financial assets, such as digital cash, cryptocurrency, and investments, all in one place. Wallet services can be provided by banks, payment processors, and third-party companies.</p><p>The wallet service is <strong>mission-critical</strong> because it is used to manage users' money and financial data, which is of utmost importance to them. Any errors or failures in the wallet service could result in financial losses or other negative consequences for users.</p><p>In addition, the wallet service is often used in <strong>real-time</strong>, which means that users need to be able to access and use the service at any time without delay. For example, if a user wants to make a payment or transfer funds, they need the wallet service to be fast and responsive to complete the transaction quickly and accurately.</p><p>As the <strong>popularity</strong> of online wallet services continues to grow, more users are relying on them for managing their financial transactions, making payments, and transferring funds. This increased demand puts pressure on the service to deliver a fast and reliable user experience. If the service cannot handle the load, it can result in slow transaction processing times, timeouts, and system crashes, leading to a poor user experience and loss of business.</p><p>When designing a wallet service, several key factors need to be considered.</p><p><strong>Functional</strong>:</p><ol><li><strong>Payment processing</strong>: Ensures that money movement or payment is processed without duplication or loss.</li><li><strong>Account Creation and Management</strong>: Allows users to create and manage accounts, providing a single platform for managing all accounts and reducing the risk of errors and discrepancies.</li><li><strong>Account Hierarchy View</strong>: Allows users to create a tree-style structure of accounts, making it easier to organize and track transactions across multiple accounts. Provides a single view of the total balance across all accounts and sub-accounts, allowing users to assess their financial position quickly.</li><li><strong>Transaction History and Auditing</strong>: The wallet service should have an accurate and immutable history as the source of truth,  providing explainability and traceability for auditing or regulatory requirements.</li></ol><p><strong>Non-Functional</strong>:</p><ol><li><strong>Availability</strong>: Provides redundancy and failover mechanisms to ensure system availability and data integrity in the event of technical issues.</li><li><strong>Low-latency</strong>: Provides real-time processing of transactions, enabling users to make payments and transfers quickly and efficiently.</li><li><strong>Scalability</strong>: Implements a distributed architecture to ensure the system can handle increased traffic and demand as the user base grows.</li><li><strong>Evolvability</strong>: Decouples business logic and fundamental APIs, making the system easily adapt to changing business requirements or customer needs without requiring a complete overhaul of the entire system.</li></ol><p>In this article, we will dive into the distributed wallet service implemented in Airwallex, examining its architecture, detailed design, and performance.</p><h1>2. Architecture and Design Decisions</h1><p>As depicted in the following diagram, the distributed wallet service is divided into three layers:</p><ul><li>The Access Layer which translates incoming flexible business requests into underlying stationary account operations,</li><li>The Transaction Layer, a.k.a. Marker, which orchestrates cross-shard money movements with ACID guarantees, where ACID refers to the four key properties of a transaction: atomicity, consistency, isolation, and durability.</li><li>The Storage Layer, a.k.a. Auticuro, which supports low-level, high-performance atomic account operations within a single shard.</li></ul><p><img loading="lazy" alt="image info" src="/Auticuro/assets/images/High_level_architecture-3817d0d4cf553f9ff60f7bc9bef43eb6.svg" width="520" height="340" class="img_ev3q"></p><p>Shards within the same layer do not communicate, while shards from different layers are fully connected. For example, Replicas in Access Layer can talk with any shard from the Transaction Layer or Storage Layer, and shards in the Transaction Layer can talk with any shard from Storage Layer.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-decouple-transaction-layer-and-storage-layer">2.1 Decouple Transaction Layer and Storage Layer<a href="#21-decouple-transaction-layer-and-storage-layer" class="hash-link" aria-label="Direct link to 2.1 Decouple Transaction Layer and Storage Layer" title="Direct link to 2.1 Decouple Transaction Layer and Storage Layer">​</a></h2><p>The most important design decision we made was to decouple the transaction layer and storage layer. The decoupling dramatically decreases the engineering complexity and maintenance overhead while increasing the performance of transaction management and account operations.</p><p>The transaction layer is computation-intensive, while the storage layer is data-intensive. The transaction layer should be scaled up and down according to the traffic volume, while the storage layer should be scaled up according to the number of accounts. A new shard in the transaction layer can be brought up when encountering traffic spikes and decommissioned as long as it has no ongoing transactions.</p><p>A shard in the storage layer with a naive single-threaded implementation could easily achieve more than 20,000 account operations per second. In contrast, a carefully designed and implemented shard in the transaction layer can schedule 2,000 transactions per second.</p><p>According to our prior experience, the monolithic solution have challenges including:</p><ul><li>Hard to implement and evolve in terms of engineering</li><li>Much slower than the decoupled solution since it needs to handle quite a few complex situations relating to the pending conditional transfer due to the coupling of transaction management and account operation</li><li>Hard to smoothly scale up and down when encountering traffic spikes since any scaling would require repartitioning</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-decouple-flex-and-stationary">2.2 Decouple Flex and Stationary<a href="#22-decouple-flex-and-stationary" class="hash-link" aria-label="Direct link to 2.2 Decouple Flex and Stationary" title="Direct link to 2.2 Decouple Flex and Stationary">​</a></h2><p>The wallet service separates the fundamental balance operation APIs from the business-facing APIs; the former is implemented in the storage layer, and the latter is implemented in the access layer. It offers several benefits:</p><ul><li>Independent Scalability: Decoupling the layers allows each layer to scale independently based on its specific requirements. The fundamental balance operation APIs are stable and should be changed conservatively, while the business-facing APIs need to evolve quickly to adapt to the business requirements.</li><li>Easier to test and debug: Separating the core balance operation APIs from the business-facing APIs makes it simpler to test and debug each layer independently, ensuring a more robust and reliable system.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-decouple-read-and-write---cqrs-with-event-sourcing">2.3 Decouple Read and Write - CQRS with Event Sourcing<a href="#23-decouple-read-and-write---cqrs-with-event-sourcing" class="hash-link" aria-label="Direct link to 2.3 Decouple Read and Write - CQRS with Event Sourcing" title="Direct link to 2.3 Decouple Read and Write - CQRS with Event Sourcing">​</a></h2><p>Command and Query Responsibility Segregation (CQRS) means separating reads and writes into different models, using commands to update data and queries to read data. Commands represent actions to change the state of the system. Usually, we refer to the command as the write operation. We would prefer the use of command and will use it throughout this document. You can think they are the synonyms of the write operations. Benefits of CQRS include:</p><ul><li>Independent scaling. CQRS allows the read and write workloads to scale independently and results in fewer lock contentions.</li><li>Optimized data schemas. The query side can use a schema optimized for queries, while the command side uses a schema optimized for updates.</li><li>Separation of concerns. Segregating the command and query sides can result in models that are more maintainable and flexible. Additionally, the command and query sides could have different optimization and scaling solutions.</li></ul><p>Instead of storing just the current state of the data in a domain, in Event Sourcing, an append-only store is used to record the full series of actions taken on that data. The store acts as the golden source, and the records can be used to materialize the domain objects.</p><p>Marker and Auticuro follow the CQRS pattern with Event Sourcing, each shard of which is separated into the command side and query side, connected by an event store. Only the most critical logic, such as real-time balance check, is processed on the command side. Others are processed on the query side, tailored for query performance.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-decouple-account-structure-and-storage">2.4 Decouple Account Structure and Storage<a href="#24-decouple-account-structure-and-storage" class="hash-link" aria-label="Direct link to 2.4 Decouple Account Structure and Storage" title="Direct link to 2.4 Decouple Account Structure and Storage">​</a></h2><p>The wallet service supports account hierarchy - a tree-like structure where an account can have a bunch of sub-accounts, and the account's balance is the total balance of its sub-accounts.</p><ul><li>Money movements are only allowed on the leaf accounts</li><li>Balance queries of all accounts(leaf account + non-leaf accounts) are supported.</li></ul><p>We decouple the two functionalities with CQRS methodology, putting the leaf account storage model on the command (write) side and the account hierarchy models on the query side because these two models are orthogonal. In this way, they can scale or adapt to business change independently.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="25-correctness-dependability-and-performance">2.5 Correctness, Dependability, and Performance<a href="#25-correctness-dependability-and-performance" class="hash-link" aria-label="Direct link to 2.5 Correctness, Dependability, and Performance" title="Direct link to 2.5 Correctness, Dependability, and Performance">​</a></h2><p>The wallet service is implemented in the Rust programming language to achieve correctness and performance. Rust’s rich type system and ownership model guarantee memory safety and thread safety. Rust is blazingly fast and memory efficient: with no runtime or garbage collector, powering performance-critical services.</p><p>The wallet service employs a single-threaded, lockless critical path to reduce contention, improve throughput, and reduce latency. This choice dramatically reduces the engineering complexity as error-prone multi-threaded code is discarded without hurting the performance.</p><p>The command side of Marker and Auticuro is built on top of the <a href="https://raft.github.io/" target="_blank" rel="noopener noreferrer">Raft algorithm</a>, leveraging Raft to achieve strong consistency and high availability. Raft is a consensus algorithm, which is a protocol used in distributed systems to achieve agreement among multiple nodes on a specific value or state, even in the presence of failures. Raft achieves consensus via an elected leader. A server in a raft cluster is either a leader or a follower and can be a candidate in the precise case of an election (leader unavailable). The leader is responsible for log replication to the followers.</p><h1>3. Detail Design</h1><p>We’ve explained the high-level architecture and design decisions of the wallet service, now let’s dive into the detailed designs. We will go through the design of the storage layer, the transaction layer, and cross-shard money movements respectively.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-storage-layer">3.1 Storage Layer<a href="#31-storage-layer" class="hash-link" aria-label="Direct link to 3.1 Storage Layer" title="Direct link to 3.1 Storage Layer">​</a></h2><p>As mentioned above, Auticuro is the storage layer of the wallet service. It provides account management and balance operation functionalities used to build composite money movement functionalities that businesses require.</p><p>Auticuro has predictable low latency (P99 <!-- -->&lt;<!-- --> 20 ms when TPS = 10,000, tested against a single Auticuro shard with a 5-node deployment on GCP) and high availability (RTO <!-- -->&lt;<!-- -->= 4s for fault recovery), making it a suitable cornerstone of critical financial applications.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="311-details-of-an-account">3.1.1 Details of an Account<a href="#311-details-of-an-account" class="hash-link" aria-label="Direct link to 3.1.1 Details of an Account" title="Direct link to 3.1.1 Details of an Account">​</a></h3><p>An account is divided into four parts: TransactionSection, Available, ReservedSection, and Configuration.</p><p><img loading="lazy" alt="image info" src="/Auticuro/assets/images/account_structure-4bec1330997b434eeaeec0519f1a6221.svg" width="640" height="601" class="img_ev3q"></p><p>TransactionSection is a mapping from transactionId to its PendingIn amount and PendingOut amount manipulated by the TCC interfaces explained later in section 3.3.2.</p><p>ReservedSection is a mapping from reservationId to its reserved amount, which supports Reserve, IncrementalReserve, Release, and PartialRelease interface.</p><p>The Reserve interface allows reserving an amount of money within an account for future usage purposes by moving the money from the available section to the reserved section. The only permitted operation on that reserved money is the Release interface which moves the money back to the available section. IncrementalReserve and PartialRelease are just variants of Reserve and Release.</p><p>The Configuration contains fields like UpperLimit, LowerLimit, State, Currency, and Version, which could be updated by UpdateUpperLimit, UpdateLowerLimit, LockAccount, and UnlockAccount interface.</p><p>The Transfer interface is a bilateral money movement between two accounts inside the same Auticuro shard. BatchBalanceOperation is a batch of money movement for accounts inside the same Auticuro shard, and each money movement credits or debits an amount from one account.</p><p>Updating the account follows a Copy-On-Write pattern. Instead of directly updating the account, each modification will be applied to a copy of the account. Each time the copy-on-write happens, we will assign an increased version number to it. so we know the entire history of the account modification.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="312-cqrs-with-event-sourcing">3.1.2 CQRS with Event Sourcing<a href="#312-cqrs-with-event-sourcing" class="hash-link" aria-label="Direct link to 3.1.2 CQRS with Event Sourcing" title="Direct link to 3.1.2 CQRS with Event Sourcing">​</a></h3><p><img loading="lazy" alt="image info" src="/Auticuro/assets/images/CQRS_with_event_sourcing-0a33ba2e56411590867cae3556b9edf7.svg" width="1020" height="601" class="img_ev3q"></p><p>Above is the high-level architecture of one Auticuro shard, which uses CQRS with Event Sourcing (defined in section 2.3).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3121-command-side">3.1.2.1 Command Side<a href="#3121-command-side" class="hash-link" aria-label="Direct link to 3.1.2.1 Command Side" title="Direct link to 3.1.2.1 Command Side">​</a></h4><p>The command side of the Auticuro shard is written in Rust to achieve high performance and correctness and uses Raft to achieve dependability under cloud environments. It processes requests for account management and balance operation, supports real-time balance checks in critical-path, and generates event logs streamed to the query side in a real-time manner.</p><p>Auticuro leverages its single-threaded critical path and Copy-On-Write pattern to achieve all-or-nothing semantics for a batch of operations. If any of the operations in a batch fails due to balance limitation checks or improper account state, the preceding changes made by that batch on cloned accounts are discarded, leaving the state unchanged.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3122-query-side">3.1.2.2 Query Side<a href="#3122-query-side" class="hash-link" aria-label="Direct link to 3.1.2.2 Query Side" title="Direct link to 3.1.2.2 Query Side">​</a></h4><p>The query side provides materialized views of the accounts by replaying event logs tailored to flexible business requirements to maximize the query performance. These views are read-only caches of the events, while the event store is the golden source of truth. It is essential that the command side generates events with a consecutive sequence number, which can be used by query sides to perform integrity checks and detect missing or out-of-order events. When the query side system is damaged, its state can be restored by replaying all past account/balance change events, and snapshots are used to accelerate the process. Typical query side services are as follows:</p><p><strong>Account Hierarchy Service</strong></p><p>Our client could set up multiple accounts for a specific business requirement and organize them as a tree-style hierarchy. Leaf accounts support modifications such as account management and balance operations, while non-leaf accounts are unmodifiable and illustrate an aggregated read-only view.</p><p>An Account Hierarchy Service is built to satisfy the above requirement:</p><ul><li>The Account Hierarchy Service applies events and calculates leaf accounts' balance</li><li>Users could CRUD account hierarchy configs via the UI, where CRUD is the acronym for CREATE, READ, UPDATE, and DELETE.</li><li>When receiving a query request, the Account Hierarchy Service reads the account hierarchy config from the database and calculates the non-leaf account's balance by aggregating the balance of its children recursively.</li></ul><p><strong>Account Query Service</strong></p><p>The Account Query Service provides the Read-Your-Write consistency query for account balances and balance change history.  After you've updated the account, it is very natural that if you immediately read it back, you should read your last modification. This is called read-your-write consistency. It is desirable because it provides a more intuitive experience for users and can help ensure correct application behavior.</p><ul><li>The Account Query Service applies events to build every version for every account.</li><li>Each account instance contains a version number, facilitating account-wise pagination queries for change history.</li><li>The versioned accounts are replicated to a data warehouse like BigQuery(an enterprise data warehouse product of Google) for OLAP analysis.</li></ul><p><strong>Kafka Connector</strong></p><p>The events are published to Kafka for downstream systems to subscribe from.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3123-fault-tolerance">3.1.2.3 Fault Tolerance<a href="#3123-fault-tolerance" class="hash-link" aria-label="Direct link to 3.1.2.3 Fault Tolerance" title="Direct link to 3.1.2.3 Fault Tolerance">​</a></h4><p>All components in the above Auticuro shard are fault tolerant. Let's analyze how to achieve fault tolerance from the write side to the read side.</p><p><strong>Command Side</strong></p><ul><li><p>Network/Pod Failure</p><p>A typical deployment of one Auticuro shard consists of 5 replicas(1 leader + 4 followers), which is available as long as the majority (n &gt;= 3 in this case) are alive. If a follower undergoes a transient network failure or pod failure, the service would not be affected. Meanwhile, the failed follower would catch up when it recovers. If the leader fails, a new leader will be elected among the remaining 4 followers, and the RTO (Recovery Time Objective) is less than 4 seconds.</p></li><li><p>Disk Failure</p><p>Auticuro periodically snapshots its state and uploads generated snapshots to cloud object storage, accelerating the recovery when a replica undergoes disk failure.</p></li><li><p>Error Detection</p><p>A recon engine incrementally pulls the raft log and event log from all 5 replicas in the background and compares them to detect potential divergence of the state machine. If divergence is detected, fault handling like rollback/roll forward is performed.</p></li></ul><p><strong>Event Store</strong></p><p>Databases like PostgreSQL store recently pulled events and backups them to cloud object storage via CDC, where CDC, a.k.a. Change Data Capture, refers to the process of identifying and capturing changes made to data in a database and then delivering those changes in real-time to a downstream process or system.</p><p>If RDBMS undergoes temporary failure, services on the query side fall back to query events directly from Auticuro. If RDBMS undergoes permanent failure, the event store recovers historical events from cloud object storage and recent events from Auticuro.</p><p><strong>Query Side</strong></p><p>Services on the query side are stateless deployment in k8s, recovering from pod failure by rebuilding the states from the event log.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="313-how-does-auticuro-work">3.1.3 How does Auticuro work?<a href="#313-how-does-auticuro-work" class="hash-link" aria-label="Direct link to 3.1.3 How does Auticuro work?" title="Direct link to 3.1.3 How does Auticuro work?">​</a></h3><p><img loading="lazy" alt="image info" src="/Auticuro/assets/images/firm-wallet-c-side-0995d4fadf5fb9135f833182d886e635.svg" width="940" height="841" class="img_ev3q"></p><p>The above figure shows how a TransferRequest is handled inside the leader of one Auticuro shard, supporting real-time balance checks in the critical path:</p><p>The TransferRequest is persisted into the raft log unconditionally, even if it is a duplicated one. In real life, one raft log entry may consist of a batch of transfer requests to amortize the time cost of permanently writing the data to disk, done by the system call fsync of Linux/Unix.</p><p>The event log stores the balance change event generated by the transfer request. Each event is assigned a consecutive integer starting from 1, used by downstream systems to verify the integrity of the event stream. A duplicated transfer request in the raft log will be detected by DedupId Store and discarded, leaving the BalanceMap unchanged and no event generated.</p><p><strong>Thread 1: The Foreground gRPC Thread</strong></p><ul><li><strong>Step 1</strong>. The gRPC thread accepts a TransferRequest, appends it to the raft log, waits for that request to be committed, and receives the corresponding raft log_index (This implies that the log entry has been persisted on the majority nodes and is ready to be applied).</li><li><strong>Step 2</strong>. The gRPC thread registers the TransferRequest into the MessageBroker with that log_index and waits for the TransferResponse from the MessageBroker and replies that response to the client.</li></ul><p><strong>Thread 2: Single-Threaded Log Consumer</strong></p><ul><li><strong>Step 1</strong>. The LogConsumer polls that committed log entry from the raft log and deserializes the log entry to the TransferRequest.</li><li><strong>Step 2</strong>. The LogConsumer sends the TransferRequest to the WalletStateMachine via the Single Producer Single Consumer (SPSC) channel.</li></ul><p><strong>Thread 3: Single-Threaded Lockless Critical Path</strong></p><ul><li><strong>Step 1</strong>. The WalletStateMachine receives the TransferRequest from the SPSC channel, de-duplicates via DedupId Store, updates the BalanceMap, generates the TransferResponse and BalanceChangeEvent, and persists both BalanceChangeEvent and dedupId into RocksDB.</li><li><strong>Step 2</strong>. The WalletStateMachine registers the TransferResponse into the MessageBroker with that log_index.</li></ul><p><strong>Thread 4: Single-Threaded Message Broker</strong></p><ul><li><strong>Match</strong>. The MessageBroker is an infinite loop matching the TransferRequest from the gRPC thread and the TransferResponse from the WalletStateMachine via the log_index.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-transaction-layer">3.2 Transaction Layer<a href="#32-transaction-layer" class="hash-link" aria-label="Direct link to 3.2 Transaction Layer" title="Direct link to 3.2 Transaction Layer">​</a></h2><p>Marker is a dependable, lightweight, scalable, and distributed Transaction Manager built from scratch for our distributed wallet service that could scale to 1,000,000 TPS.</p><p>Marker provides ACID guarantees and predictable latency for cross-shard money movements by orchestrating them as a Directed Acyclic Graph (DAG), following the topological order explained later.</p><p>Marker uses the same CQRS pattern with Event Sourcing (see section 2.3) as the storage layer Auticuro, so they share the same engineering advantages.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="321-concepts-of-workflow">3.2.1 Concepts of Workflow<a href="#321-concepts-of-workflow" class="hash-link" aria-label="Direct link to 3.2.1 Concepts of Workflow" title="Direct link to 3.2.1 Concepts of Workflow">​</a></h3><p><img loading="lazy" alt="Concepts of Workflow" src="/Auticuro/assets/images/concepts_of_workflow-d372379ce6dad7b89c7d1981738f40b5.svg" width="1160" height="1101" class="img_ev3q"></p><p>The basic execution unit inside Marker is a Workflow Instance that is a DAG, whose vertex is a Job and whose edge is a Dependency between two Jobs.</p><p>Jobs are scheduled according to topological order: a Job is available for scheduling when all of its incoming Dependencies are fulfilled, and concurrent Jobs are scheduled parallelly yet completed sequentially.</p><p>Here are some fundamental concepts:</p><ul><li>Context: a private JSON Object shared by all the Jobs of a workflow instance.</li><li>JobRequest: the request sent to the worker, built from the Context.</li><li>JobResponse: the response received from the worker, used to update the Context.</li><li>Precondition: a boolean expression evaluated on fields inside the Context set by preceding Jobs.</li></ul><p>A dependency without a Precondition is fulfilled as long as its predecessor Job is finished, while a dependency with a Precondition is fulfilled when its predecessor Job is finished and its Precondition is fulfilled.</p><p>A Workflow Instance achieves its finished state when no more available Jobs are ready for scheduling.</p><p>The lifecycle of a Job is:</p><ul><li>Pending: not all incoming dependencies fulfilled,</li><li>Available: all incoming dependencies fulfilled, ready for scheduling,</li><li>Activated: Job is assigned with JobRequest sent,</li><li>Finished: Job is finished with JobResponse received.</li></ul><p>Marker uses the CQRS pattern with event sourcing. With this pattern, the state of a workflow instance is stored as a sequence of events. Each event represents a set of changes to the workflow instance. The current state is constructed by replaying the events.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="322-cqrs-with-event-sourcing">3.2.2 CQRS with Event Sourcing<a href="#322-cqrs-with-event-sourcing" class="hash-link" aria-label="Direct link to 3.2.2 CQRS with Event Sourcing" title="Direct link to 3.2.2 CQRS with Event Sourcing">​</a></h3><p><img loading="lazy" alt="one_marker_shard" src="/Auticuro/assets/images/one_marker_shard-15fd287b92724606a23e6fe9a3f4ea9e.svg" width="930" height="570" class="img_ev3q"></p><p>The above graph is a high-level architecture of one Marker shard, which follows the CQRS pattern with Event Sourcing.</p><p>The center part is the Command Side (Core Service that uses the Raft algorithm to achieve Dependability). The left part is the Workers integrated with Marker and Services embedding the Marker Client Lib. The right part is the Query Service which pulls the raft log, rebuilds the Marker StateMachine by replaying the raft log, and serves the OLTP query requests.</p><p>Here is the interaction between components when handling a business request:</p><ul><li>Gateway enriches the business request into a CreateWorkflow request according to Service Orchestration Rules and forwards it to Core Service.</li><li>Core Service schedules the Jobs of the workflow in topological order by sending AssignsJob requests to workers and waits for FinishJob requests from workers.</li><li>Query Service pulls the event log, rebuilds the workflow's states in real time, and serves the query request from Gateway.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="323-how-does-marker-work">3.2.3 How does Marker work?<a href="#323-how-does-marker-work" class="hash-link" aria-label="Direct link to 3.2.3 How does Marker work?" title="Direct link to 3.2.3 How does Marker work?">​</a></h3><p><img loading="lazy" alt="how_does_marker_work" src="/Auticuro/assets/images/how_does_marker_work-15753027437bd7502a96394e980a6abf.svg" width="947" height="540" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3231-marker-statemachine">3.2.3.1 Marker StateMachine<a href="#3231-marker-statemachine" class="hash-link" aria-label="Direct link to 3.2.3.1 Marker StateMachine" title="Direct link to 3.2.3.1 Marker StateMachine">​</a></h4><p>A Marker shard uses both memory and disk to hold unfinished workflows. Workflows are handled in a FIFO order. If the number of unfinished workflows exceeds a predefined threshold, the newly created workflows will spill to disk. Here are the components of the Marker StateMachine:</p><ul><li>ActiveWorkflows: an in-memory queue holding the workflows under scheduling.</li><li>PendingWorkflowQueue: a disk queue holding the spilled workflows.</li><li>FinishedWorkflows: a disk-based storage engine, e.g. RocksDB, holding recently finished workflows to facilitate the QueryService.</li><li>AvailableJobs: a mapping from a workflow to its Jobs in the Available state,</li><li>ActivatedJobs: a mapping from a Worker to the Jobs assigned to that worker</li></ul><p>If a worker crashes while handling these Jobs, Marker will help the worker to recover its ongoing Jobs when it registers with Marker again, which makes stateless service a good adoption for workers.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3232-threading-model">3.2.3.2 Threading Model<a href="#3232-threading-model" class="hash-link" aria-label="Direct link to 3.2.3.2 Threading Model" title="Direct link to 3.2.3.2 Threading Model">​</a></h4><p><strong>ApplyLoop</strong> is the single-threaded critical path that reads committed raft log entries and applies them to the Marker StateMachine. This thread runs in the leader and followers parallelly to build the replicated state machines to achieve dependability.</p><p>There are 3 types of events (CreateWorkflowEvent, AssignJobEvent, and FinishJobEvent). They are persisted into the raft log unconditionally, while before applying them, ApplyLoop will check the Marker StateMachine to discard the duplicated and/or stale ones.</p><p>When running in the leader, <strong>ProcessLoop</strong> is the single-threaded critical path that handles the newly generated AvailableJobs and ActivatedJobs emitted by Marker StateMachine. For AvailableJobs, it checks the workers registered in the leader, picks one with a matching job type, and tries to persist an AssignJobEvent into the raft log so that this assignment can survive failures. For ActivatedJobs, it fetches the gRPC channel of the assigned worker and sends the JobRequest to that worker via the channel.</p><p>When running in followers, ProcessLoop evicts their registered workers, forcing them to re-register with the leader eagerly.</p><p><strong>gRPC Threads</strong> persist CreateWorkflowEvent and FinishJobEvent into the raft log unconditionally, even though these events may be duplicated and/or stale, which will be detected and discarded by ApplyLoop before applied to the Marker StateMachine.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3233-life-of-a-simple-workflow">3.2.3.3 Life of a simple workflow<a href="#3233-life-of-a-simple-workflow" class="hash-link" aria-label="Direct link to 3.2.3.3 Life of a simple workflow" title="Direct link to 3.2.3.3 Life of a simple workflow">​</a></h4><div style="text-align:center"><p>  <img loading="lazy" alt="image" src="data:image/svg+xml;base64,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" width="210" height="120" class="img_ev3q"></p></div><ul><li><strong>Step 1</strong>: The client sends a CreateWorkflow request for the above sample workflow that consists of 2 Jobs A and B.</li><li><strong>Step 2</strong>: A CreateWorkflowEvent is persisted into the raft log unconditionally.</li><li><strong>Step 3</strong>: After that log entry is committed, ApplyLoop applies it to the Marker StateMachine:<ul><li>One active workflow is created,</li><li>Job A transitions to Available since it has no incoming dependency,</li><li>Marker StateMachine emits Job A as the newly generated AvailableJobs.</li></ul></li><li><strong>Step 4</strong>: ProcessLoop picks one worker for Job A, and An AssignJobEvent is persisted into the raft log unconditionally.</li><li><strong>Step 5</strong>: After that log entry is committed, ApplyLoop applies it to the Marker StateMachine:<ul><li>Job A transitions to Activated</li><li>Marker StateMachine emits Job A as the newly ActivatedJobs.</li></ul></li><li><strong>Step 6</strong>: ProcessLoop fetches the gRPC channel of the assigned worker and sends the JobRequest to that worker via the channel.</li><li><strong>Step 7</strong>: The worker sends a FinishJob request for Job A after finishing Job A.</li><li><strong>Step 8</strong>: A FinishJobEvent is persisted into the raft log unconditionally.</li><li><strong>Step 9</strong>: After that log entry is committed, ApplyLoop applies it to the Marker StateMachine:<ul><li>Job A transitions to Finished, Dependency <!-- -->&lt;<!-- -->A, B&gt; Fulfilled, Job B transitions to Available.</li><li>Marker StateMachine emits Job B as the newly generated AvailableJobs.</li></ul></li><li>Omit the remaining steps since the Handling of Job B is the same as that of Job. A.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-cross-shard-money-movements">3.3 Cross-Shard Money Movements<a href="#33-cross-shard-money-movements" class="hash-link" aria-label="Direct link to 3.3 Cross-Shard Money Movements" title="Direct link to 3.3 Cross-Shard Money Movements">​</a></h2><p>Cross-shard money movements are implemented as distributed transactions with ACID guarantees. Our distributed wallet service supports two kinds of distributed transactions: Saga and TCC.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="331-saga">3.3.1 Saga<a href="#331-saga" class="hash-link" aria-label="Direct link to 3.3.1 Saga" title="Direct link to 3.3.1 Saga">​</a></h3><p>The Saga pattern provides transaction management using a sequence of local transactions. A local transaction is the atomic work effort performed by a saga participant. If a local transaction fails, the saga executes a series of compensating transactions that undo the changes made by the preceding local transactions.</p><p>In our distributed wallet service, any Marker shard is an independent saga transaction manager, while all Auticuro shards are saga participants that support compensating local transactions. Besides the basic saga pattern, our distributed wallet service implements a saga pattern with optimistic locking.</p><p>Optimistic locking is a concurrency control method that assumes that conflicts between multiple transactions are rare. Instead of locking the resources for the entire transaction duration, optimistic locking allows multiple transactions to access the same resource simultaneously. Before committing a transaction, the system checks if the resource has been modified by another transaction since it was last read. If the resource has been modified, the transaction fails, and the client must retry the transaction.</p><p><img loading="lazy" alt="saga" src="/Auticuro/assets/images/saga-05af2ae913fa9c2ea8deb124b8a847cf.svg" width="980" height="540" class="img_ev3q"></p><p>Here is an example of a saga with optimistic locking. The client has three cards A, B, and C. A and B are credit cards with a maximum balance of 0, and C is a debit card with a minimum balance of 0. Before top-up, the balance of A, B, and C is -30, -20, and 0. Now our client wants to deposit 100 with the expectation that pay the debt in credit cards A and B first, then deposit the remaining to debit card C.</p><p>The execution is divided into two phases. In the first phase, the gateway queries the balances of A, B, and C. In the second phase, the gateway builds up an execution plan:</p><ul><li>Deposit 30 to card A with the expectation that A has a balance of -30 before top-up</li><li>Deposit 20 to card B with the expectation that B has a balance of -20 before top-up</li><li>Deposit 50 to card C unconditionally</li></ul><p>During execution, any concurrent changes to A, B, and C will lead the saga transaction to fail and roll back, for example:</p><ul><li>The client withdraws 10 from credit card B(changing the balance of B from -20 to -30) after the execution of the first phase, then during the second phase, the deposit of B will fail due to unsatisfied expectations on the balance of B, since it is not acceptable that balance of B is -10 while that of C is 50 (you should always pay the debt first).</li><li>The client deposits 10 to credit card A(changing the balance of A from -30 to -20) after the execution of the first phase, then during the second phase, the deposit of A will fail due to unsatisfied expectations on the balance of A, since the maximum balance of credit card is 0.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="332-tcc">3.3.2 TCC<a href="#332-tcc" class="hash-link" aria-label="Direct link to 3.3.2 TCC" title="Direct link to 3.3.2 TCC">​</a></h3><p>TCC, Try/Confirm/Cancel, is a two-phase commit protocol that divides the commitment of transactions into two phases:</p><ol><li>In phase one, TCC performs business checks (for consistency) and reserves business resources (for quasi-isolation). This defines the try operation of TCC.</li><li>In phase two, if the reservation of all business resources is successful in the try phase, the confirm operation is performed. Otherwise, the cancel operation is performed.</li></ol><p>The confirm operation acts only on the reserved resources without business checks. If the operation fails, the system keeps retrying. The cancel operation cancels the execution of a business operation, releases the reserved resources, and retries if it fails.</p><p>In our distributed wallet service, any Marker shard is an independent transaction coordinator, while all Auticuro shards are transaction participants that support the Try, Confirm, and Cancel interface.</p><p><img loading="lazy" alt="tcc" src="/Auticuro/assets/images/tcc-2fa821c49563a99bb8db55889fe7b564.svg" width="881" height="611" class="img_ev3q"></p><p>Here is an example of cross-shard money movements: A - 10, B - 10, C + 20, while A, B, and C are located in different Auticuro shards.</p><p>When Gateway(the Access Layer) receives such a request, it builds a workflow instance as above and sends it to a Marker shard routed by the hash of the workflow id. That Marker shard schedules the execution of the Jobs according to Topological Order.</p><p>In Phase 1, three Jobs TryA, TryB, and TryC are scheduled in parallel.</p><p>Jobs like SyncPoint, ConfirmAll, and CancellAll are internal Jobs executed independently inside Marker. Job ConfirmAll or CancelAll is triggered by evaluating the Precondition on JobResponse of TryA, TryB, and TryC.</p><p>In Phase 2, ConfirmAll triggers the parallel execution of ConfirmA, ConfirmB, and ConfirmC, while CancelAll triggers the parallel execution of CancelA, CancelB, and CancellC. Each of the Jobs retries until succeeding.</p><h1>4. Evaluation</h1><p>Comprehensive tests are conducted against the <strong>single-shard Auticuro </strong>to assess its correctness, robustness, and performance.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-chaos-test">4.1 Chaos Test<a href="#41-chaos-test" class="hash-link" aria-label="Direct link to 4.1 Chaos Test" title="Direct link to 4.1 Chaos Test">​</a></h2><p>Chaos test, also known as fault injection, in which we intentionally introduce failures into a system to evaluate its ability to maintain function and recover gracefully.</p><p>The test deployment:</p><ul><li>A 5-replica <strong>Auticuro</strong> k8s stateful set on the GCP</li><li>A request sender that keeps sending requests to the Auticuro @ 200TPS</li><li>A consistency checker that checks the consistency of the raft/event logs and wallet state machine among the five replicas.</li></ul><p>Chaos test scenarios:</p><ul><li>Injecting pod failure in a round-robin way every 10 minutes</li><li>Injecting network loss failure in a round-robin way every 10 minutes</li></ul><p>Test Result:</p><ul><li>More than <strong>100M</strong> requests are processed by the Auticuro cluster without duplicate or loss, during which there are more than <strong>1000</strong> times pod/network chaos happened.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-stress-test">4.2 Stress Test<a href="#42-stress-test" class="hash-link" aria-label="Direct link to 4.2 Stress Test" title="Direct link to 4.2 Stress Test">​</a></h2><p>We use 8 vCPUs * 5 node clusters on GCP, which are attached with SSD persistent disks (pd-ssd). The<strong> P99 latency <!-- -->&lt;<!-- --> 20ms</strong> when <strong>TPS = 10K. </strong></p><p>Different workloads are tested, and the detailed latency distribution vs. throughput is shown below:
<img loading="lazy" alt="image info" src="/Auticuro/assets/images/latency_distribution-760693de9bb234a9003851c00c39a4f1.gif" width="1280" height="960" class="img_ev3q"></p><p>In summary, Auticuro has proven to be a low-latency, high-performance, and reliable storage engine that suits mission-critical financial applications. We aim to provide an exceptional wallet service solution and contribute to the fintech industry and open-source community by sharing our insights, knowledge, and experiences.</p>]]></content:encoded>
        </item>
    </channel>
</rss>